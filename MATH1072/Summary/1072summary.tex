\documentclass{article}
\usepackage[margin=0.7in]{geometry}

\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{pdflscape}
\usepackage{lipsum}
\usepackage{multicol}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\theoremstyle{remark}
\newtheorem{remark}{Remark}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}

\begin{document}
\begin{landscape}
  \begin{multicols*}{3}
    [
    \section*{MATH1072 Summary - Ordinary Differential Equations}
    ]
 \section*{Dimensional Analysis}
 \subsection*{Base Dimensions}
 Majority of dimensions consist of the main base dimensions:
 \begin{itemize}
   \item Length \( (L) \)
   \item Time \( (T) \)
   \item Mass \( (M) \)
 \end{itemize}
 There are other non-mechanical base dimensions like temperature, electrical
 current etc.
 \subsection*{Derived Dimensions}
 Some common derived dimensions include
 \begin{itemize}
   \item Speed - \( \displaystyle \frac{L}{T} \)
   \item Force - \( N = M \displaystyle \frac{L}{T^2} \)
   \item Energy - \( J = M \displaystyle \frac{L^2}{T^2} \)
 \end{itemize}
 It is important that the dimensions on both sides of the equation are
 \textbf{equal}. We then call the equation \textbf{dimensionally
 homogeneous}

 \section*{ODEs}
 \subsection*{Equilibrium Solutions}
 An equilibrium solution is a constant solution such that \( y(t) = c \)
 satisfies the ODE.\\\\
 In other words, the solution to \( f(t,y) = 0 \) where \( f(t,y) = y'\),
 representing the ODE.
 \subsubsection*{Stability of Solutions}
 If the general solutions in a small neighbourhood converge towards an
 equilibrium solution \( y = c \), then it is said to be the solution is \textit{stable} at \(
 y = c\).
 \subsection*{Analytical Solutions}
 \subsubsection*{Linear First Order ODEs}
 The key thing is if the ODE is separable, the form of a linear (first order)
 ODE is
 \[ 
   y'(t) = f(t)y(t) + g(t)
 \]
 \begin{example}
 The linear ODE \( ty' + y = t\cos t \) is separable. Note that by the chain
 rule, \( (ty)' = ty' + y \).
 \begin{align*}
   (ty)' &= t\cos t\\
   \intertext{Integrate both sides}
   \int (ty)' \; dy &= \int t\cos t \; dt\\
   \intertext{By integration by parts}
   ty &= t\sin t - \int \sin t \; dt\\
   ty &= t\sin t + \cos t + c_1\\
 \end{align*}
 Therefore our ODE solution is
 \[ 
   y = \sin t + \frac{\cos t}{t} + \frac{c_1}{t}
 \]
 \end{example}
 \subsection*{Approximating Solutions}
 \subsubsection*{Euler's Method}
 Given \( y' = f(t,y), \; y(0) = c \)
 \begin{align*}
   y'(t) &= \displaystyle \lim_{\Delta  \to 0} \frac{y(t + \Delta)
   - y(t)}{\Delta}\\
   &\approx \frac{y(t + \Delta) - y(t)}{\Delta}
 \end{align*}
 Euler's Method is iterative, so \( y_{k+1} = y_k + f(t_k, y_k) \). So
 \[ 
   \frac{y(t_{k+1}) - y(t_k)}{\Delta} \approx f(t_k, y_k)
 \]

 \subsubsection*{Heun's Method}

 \subsection*{Second Order Differential Equations}
 Say we have a linear second order ODE of form \( f(y'', y', y, t) = 0 \),
 assuming \( f \) is homogeneous and constant coefficient, we can denote the
 ODE as
 \[ 
   y'' + ay' + by = 0
 \]
 For any ODE of this form, we can represent its characteristic equation as 
 \[ 
   \lambda^2 + a\lambda + b = 0
 \]
 (You should know where this comes from, but its ok if you dont). We can solve
 the characteristic equation with the quadratic formula
 \[ 
 \lambda_{1,2} = \frac{-a \pm \sqrt{a^2 - 4b}}{2}
 \]
 \paragraph{Case 1: There are 2 real solutions for \( \lambda \)}
 The general solution is then \[ 
   y(t) = c_1 e^{\lambda_1 t} + c_2 e^{\lambda_2 t}
 \]
 \paragraph{Case 2: There is only 1 real solution for \( \lambda \)}
 The general solution is then \[ 
   (c_1t + c_2)e^{\lambda t}
 \]
 \paragraph{Case 3: There is no real solutions (only complex) for \( \lambda \)}
 As there is no real solutions, then \( \lambda = \alpha \pm \beta i\), thus
 the general solution is
 \[ 
   y(t) = e^{\alpha t}(A\cos(\beta t) + B\sin (\beta t))
 \]
 Generally analytically solvable 2nd order ODES may look like
 \[ 
   y'' + p(t)y' + q(t)y = r(x)
 \]
 If \( r(x) = 0 \), we say it is homogeneous. If it is homogeneous, we can use
 the \textbf{principle of superposition} for the general solution.
 \subsubsection*{Method of Reduction of Order}
 The steps to solve with this method are
 \begin{enumerate}
   \item y'' + p(t)y' + q(t)y = 0
   \item Assume that \( y_1(t) \) is a solution.
   \item Look for solutions of the form \( y = u(t)y_1(t) \)
   \item Substitute into the equation
   \item \( (uy_1)'' + p(t)(uy_1)' + q(t)uy_1 = 0 \)
 \end{enumerate}
 \end{multicols*}
 \newpage
 \begin{multicols*}{3}
   [
   \section*{MATH1072 Summary - Multivariate Calculus}
   ]
   \section*{Two Variable Limits}
   For a function \( f(x,y) \), we can check if the limit
   \[ 
     \displaystyle \lim_{(x,y) \to (a,b)} f(x,y)
   \]
   exists with the following method
   \[
       \text{If } \displaystyle \begin{cases}
       f(x,y) \to L_1 \text{ as } (x,y) \to (a,b) \text{ along }     C_1
       \in D\\
       f(x,y) \to L_2 \text{ as } (x,y) \to (a,b) \text { along } C_2
       \in  D
     \end{cases}
   \]
   Where \( C_1, C_2 \) are paths in the domain \( D \).\\\\
   We define multivariate limits similar to single variable limits,
   \[ 
     \displaystyle \lim_{(x,y) \to (a,b)} f(x,y) = L
   \]
   If \( \forall \varepsilon > 0, \exists \delta > 0  \) such that if \( (x,y)
   \in D\) then
   \[ 
     \sqrt{(x-a)^2 + (y-b)^2} < \delta \implies |f(x,y) - L| < \epsilon
   \]

   \section*{Partial Derivatives}
   Given a function \( f(x,y) \), the partial derivatives of \( x \) and \(
   y \) respectively at a point \( P = (a,b) \) are
   \[ 
     \frac{\partial^{}{f}}{\partial{x}^{}}(a,b) = f_x(a,b) = \displaystyle
     \lim_{h \to 0} \frac{f(a+h, b) - f(a,b)}{h}
   \]
   \[ 
     \frac{\partial^{}{f}}{\partial{y}^{}}(a,b) = f_y(a,b) = \displaystyle
     \lim_{h \to 0} \frac{f(a, b+h) - f(a,b)}{h}
   \]
   Generally, the partial derivative \( \displaystyle \frac{\partial^{}{f}}{\partial{x}^{}}
   \) or \( \displaystyle \frac{\partial^{}{f}}{\partial{y}^{}}  \) can be thought of (and computed) as the derivative with respect to \(
   x \) or \( y \) respectively.
   \begin{example}
     Find the partial derivatives of \( f(x,y) = x\sin y + y \cos x \)
   \end{example}
   \begin{align*}
     \frac{\partial^{}{f}}{\partial{x}^{}} &= \frac{\partial}{\partial{x}}\Big
   (x\sin y + y\cos x\Big)\\
   \intertext{Anything with a \( y \) is basically treated as a constant}
     &= \sin y - y \cos x\\
     \intertext{The same applies to the partial derivative of \( y \)}
     \frac{\partial^{}{f}}{\partial{y}^{}}
     &= \frac{\partial^{}{}}{\partial{y}^{}}\Big(x\sin y + y \cos x\Big)\\
     &= x\cos y + \cos x
   \end{align*}
   \subsection*{Higher Order Partial Derivatives}
   Higher order partials are usually notated as
   \[ 
     f_{xx} = \frac{\partial^{2}{f}}{\partial{x}^{2}} \qquad f_{yy}
     = \frac{\partial^{2}{f}}{\partial{y}^{2}}
   \]
   We can also have higher orders of unique partials
   \[ 
     f_{xy} = \frac{\partial^{2}{f}}{\partial{y}^{}\partial{x}}
     = \frac{\partial^{}{}}{\partial{y}^{}}\Big
   ( \frac{\partial^{}{f}}{\partial{x}^{}}\Big )
 \]
 \[
   f_{yx}
   = \frac{\partial^{2}{f}}{\partial{x}^{} \partial{y}}
   = \frac{\partial^{}{}}{\partial{x}^{}}\Big
 ( \frac{\partial^{}{f}}{\partial{y}^{}}\Big )
   \]
   If \( f_{xy} \) and \( f_{yx} \) are both continuous, then \( f_{xy}
   = f_{yx} \).
   \subsection*{Chain Rule for Partials}
   For a function \( f(x,y) \) where \( x,y \) are functions of \( t \), the
   chain rule is defined as
   \[ 
     \frac{df}{dt} = \frac{\partial^{}{f}}{\partial{x}^{}} \frac{dx}{dt}
     + \frac{\partial^{}{f}}{\partial{y}^{}} \frac{dy}{dt}
   \]
  You can extrapolate this to as many dimensions as you want, Given
  a function \( f(a_1(t), a_2(t), a_3(t) \dots , a_n(t)) \), you can represent its
  derivative as
  \[ 
    \frac{df}{dt} = \displaystyle \sum_{i=1}^{n}
    \Big( \frac{\partial^{}{f}}{\partial{a_i}^{}} \frac{da_i}{dt} \Big )
    = \frac{\partial^{}{f}}{\partial{a_1}^{}} \frac{da_1}{dt}
    + \dots
    + \frac{\partial^{}{f}}{\partial{a_n}^{}} \frac{da_n}{dt}
  \]
  Where \( n \) is the highest dimension of the function \( f \).
  \section*{Gradient Vectors \( \nabla f \)}
  The gradient vector of \( f \) is defined as
  \[ 
    \nabla f = \Big ( \frac{\partial^{}{f}}{\partial{x}^{}},
    \frac{\partial^{}{f}}{\partial{y}^{}}\Big ) = f_x \textbf{i} + f_y
    \textbf{j}
  \]
  \subsection*{Directional Derivative}
  We can express the direction derivative with the gradient vector
  \[ 
    f_u = \nabla f \cdot \frac{ \textbf{u}}{|| \textbf{u} ||}
  \]
  As this is the dot product, recall that 
  \[\underline{A} \cdot \underline{B}
  = ||A|| \;\; ||B|| \cos \theta\]
  \section*{Tangent Planes}
  For \( z = f(x,y) \) at \( (a,b, f(a,b)) \), the tangent plane is defined as
  \[ 
    f(a,b) + f_x(a,b)(x-a) + f_y(a,b)(y-b)
  \]
  \section*{Line Integrals}
  For higher dimensions, we can compute work done by a force field with a line
  integral. Given a function defined by
  \[ 
    \textbf{F}(x,y,z) = F_1(x,y,z) \textbf{i} + F_2(x,y,z) \textbf{j} + F_3
    (x,y,z) \textbf{k}
  \]
  which moves along a curve \( C \). Then we denote the line integral to be
  \[ 
    \int_C  \textbf{F} \cdot d \textbf{r} 
  \]
  Where \( \textbf{r} \) is a parametrisation of the curve \( C \).
  \[ 
    \textbf{r}(t) = (x(t), y(t), z(t)), \quad a \leq t \leq b
  \]
  To compute work is to compute the line integral given a domain \( [a,b] \) on
  \( C \).
  \[ 
    W = \int_C^{}  F \cdot d\textbf{r} = \int_a^b  F(r(t)) \cdot r'(t) \; dt
  \]
  \subsection*{Conservative Fields}
  \( \textbf{F} \) is conservative if the line integral between \( A \) and \(
  B\), \( \displaystyle \int_A^B \textbf{F}\cdot   \; d \textbf{r} \) will give
  the same result for \textbf{any} path you choose in between \( A \) and \( B \)
  \\\\
  If a gradient field \( \textbf{F} \) is conservative, then there exists
  a function \( f \) such that \( \nabla f = \textbf{F} \), which you can use
  to compute line integrals with
  \[ 
    \int_A^B  \textbf{F}\cdot  d \textbf{r} = f(B) - f(A)
  \]
  Where \( f \) is called the \textbf{potential function}. It is expected to
  use this formula as apposed to the parametrisation of the path when asked to
  evaluate the work done on a conservative field. (Or other cases of computing
  a line integral over a path)

  \subsubsection*{Checking if a field is conservative}
  An easy check to see if a force field is conservative is given \( \textbf{F}
  = (F_1, F_2) = \Big (\displaystyle \frac{\partial^{}{f}}{\partial{x}^{}},
  \frac{\partial^{}{f}}{\partial{y}^{}}\Big ) \).
  \[ 
    \frac{\partial^{}{F_1}}{\partial{x}^{}}
    = \frac{\partial^{}{F_2}}{\partial{y}^{}} \implies \text{Field is
    conservative}
  \]
  
 \end{multicols*}
 \end{landscape}
\end{document}
